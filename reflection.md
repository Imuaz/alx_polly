# Reflection: Building a Polling App with AI-Powered Tools

Using AI-powered tools like an IDE, CLI, and CodeRabbit as a code reviewer significantly shaped my development experience while building this polling app. The integration of AI into the workflow introduced both exciting possibilities and unexpected challenges, ultimately teaching me a lot about prompting, reviewing, and iterating in a modern development environment.

## What Worked Well

One of the most impressive aspects of using AI tools was the speed and convenience they brought to the development process. The AI-powered IDE helped scaffold components quickly, generate boilerplate code, and even suggest improvements in real time. CodeRabbit, in particular, was helpful in reviewing code for syntax issues, suggesting refactors, and catching edge cases I might have missed. The CLI tools streamlined repetitive tasks and allowed me to focus more on logic and structure.

Prompting the AI with clear, structured questions often led to surprisingly accurate and helpful responses. For example, when I needed help designing the database schema, the AI provided a solid starting point that I could tweak to fit my needs. It also helped me generate RESTful endpoints and guided me through setting up basic routing and state management.

## What Felt Limiting

Despite the benefits, I encountered several limitations that impacted my workflow. The free versions of some tools had usage caps or missing features, which forced me to switch between IDEs mid-project. This disrupted my flow and introduced inconsistencies in formatting and tooling.

Test suite implementation was another major hurdle. While the AI could generate test cases, they often lacked depth or failed to align with the evolving structure of my app. I still don’t have all tests passing, and debugging AI-generated tests proved more time-consuming than writing them manually in some cases.

Additionally, the AI sometimes made assumptions that didn’t match my intent, especially when prompts were vague or lacked context. This taught me the importance of being precise and iterative in my communication with the tools.

## What I Learned

This experience taught me that AI is a powerful collaborator—but not a replacement for developer intuition. Prompting is an art: the more context and clarity you provide, the better the output. I learned to iterate on prompts, refine them, and even challenge the AI’s suggestions when they didn’t feel right.

Reviewing AI-generated code also required a critical eye. While tools like CodeRabbit were helpful, they occasionally missed deeper architectural concerns or performance implications. I had to balance trust in the AI with manual validation and testing.

Overall, AI accelerated many parts of the build process, but it also introduced new layers of complexity. Switching tools, debugging generated code, and managing test coverage reminded me that human oversight is still essential. I now see AI as a force multiplier—one that works best when paired with thoughtful prompting, structured workflows, and a willingness to iterate.

This project was a valuable learning experience, and I’m excited to continue refining my approach to AI-assisted development in future builds.
